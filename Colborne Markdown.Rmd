---
title: "MMA 865 Final Project"
output: github_document
date: "Summer 2017"
author: "Team Colborne"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries}
library(tidyverse)
library(data.table)
library(caret)
library(dplyr)
library(sparklyr)


config <- spark_config() 
config$'sparklyr.shell.executor-memory' <- "20g" 
config$'sparklyr.shell.driver-memory' <- "10g" 
config$spark.yarn.am.memory <- "15g"

# Full list of config options: https://spark.apache.org/docs/2.0.1/running-on-yarn.html

sc <- spark_connect(master = "yarn-client", spark_home = "/usr/hdp/current/spark2-client/", config=config)



```



## Spark Data File

```{r}


in_path_spark = 'hdfs:///user/hpc3552/scene-csv/sample03/clean/'

scene_mbr_dim <- spark_read_csv(sc, name='scene_mbr_dim', path=paste(in_path_spark, 'scene_mbr_dim.csv', sep=""), header = TRUE, delimiter = ",")

scene_mbr_acct_dim <- spark_read_csv(sc, name='scene_mbr_acct_dim', path=paste(in_path_spark, 'scene_mbr_acct_dim.csv', sep=""), header = TRUE, delimiter = ",")

scene_pt_fact <- spark_read_csv(sc, name='scene_pt_fact', path=paste(in_path_spark, 'scene_pt_fact.csv', sep=""), header = TRUE, delimiter = ",")

#First lets reduce the dataset to what we think we need
tidy_pt<-tbl(src=sc, "scene_pt_fact") %>%
  select(txn_amt,scene_pt_tp_key, scene_mbr_acct_key, anul_clndr_code, mo_clndr_code)



#At Some Point (not sure when yet), we will need to join the datasets with the member dim tables




```





#R Environment Data File
```{r}
in_path = '/global/project/queens-mma/scene-csv/sample03/clean/'

scene_mbr_dim <-fread(paste(in_path, 'scene_mbr_dim.csv', sep=""), sep=",")
scene_mbr_acct_dim <-fread(paste(in_path, 'scene_mbr_acct_dim.csv', sep=""), sep=",")
scene_pt_fact <-fread(paste(in_path, 'scene_pt_fact.csv', sep=""), sep=",")


cols<-c("txn_src","txn_tp_1", "txn_tp_2","txn_tp_3",
        "mo_clndr_code","anul_clndr_code", "anul_fncl_code")

scene_pt_fact %>%
  select(one_of(cols))%>%
  lapply(factor)
 
 scene_pt_fact %>%
   group_by(nm)%>%
   summarise(Amounts=sum(txn_amt))%>%
   arrange(desc(Amounts))


#Some dplyr data exploration stuff
Amounts_by_transaction <- scene_pt_fact %>%
   group_by(desc)%>%
   summarise(Amounts=sum(txn_amt))%>%
   arrange(desc(Amounts))


#Fact Table

tidy_pt<-scene_pt_fact %>%
  select(txn_amt,scene_pt_tp_key, scene_mbr_acct_key, anul_clndr_code, mo_clndr_code)

#Make txn_amt NA = 0



#Member Dimension Table

member$scene_acty_stat<-as.numeric(member$scene_acty_stat)
#Lots of NAs, may have to impute these values

tidy_member<-scene_mbr_dim %>%
  select(scene_mbr_key, brth_dt, psnl_post_cd, gndr_desc, scene_acty_stat)
#Let's leave out member account data for now



#Now we need to figure out how to create a response variable. I think the best approach is to create 24 variables...one for each of the preceding 24 months spend for a given customer and use those lagged variables to account for the time-series portion of the problem
scene_pt_fact <- scene_pt_fact %>%
      mutate(txn_amt = ifelse(is.na(txn_amt),0,colname))



scene_pt_fact %>%
   group_by(scene_mbr_key,  anul_clndr_code , mo_clndr_code)%>%
   summarise(Amounts=sum(txn_amt))

scene_pt_fact %>%
   group_by(anul_clndr_code)%>%
   summarise(Amounts=sum(txn_amt)) %>%
  ggplot(aes(x=anul_clndr_code))+
  geom_boxplot()


```



```{r}



#Clean the Data for Use

#First lets engineer a response variable



class(scene_mbr_dim)


#Let's start by cleaning the data types up a bit


pt<-pt%>%
  mutate(cols=)
  

pt<-lapply(pt[,10:13], factor)

colnames(pt)

# Now lets do some Data Exploration
#First filter pt for non-outlier points
domain<-2500

clean_pt<-pt %>%
  filter(pt>-domain,
         pt<domain)
  hist(clean_pt$pt)
  
clean_txn <-pt %>%
  filter(txn_amt>0,
         txn_amt<6000)

#Some Data Viz Code
ggplot(clean_txn, aes(x=txn_amt, fill=anul_clndr_code))+geom_histogram(bins=30)

nrow(clean_txn)
nrow(pt)
#Erased 6.8 million transcations with less than 0
#This cleaning left 59.4% of the rows left (most of them had negative transactions)
pos_txn <-pt %>%
  filter(txn_amt>0)
nrow(pos_txn)
#9.9843 million rows that are positive

btwn_txn <-pos_txn %>%
  filter(txn_amt<6000)

nrow(btwn_txn)
#9.9686 million rows between domain


neg_txn<-pt[txn_amt<0,]
View(neg_txn)

count(unique(pt$txn_src))
pt$txn_src<-as.factor(pt$txn_src)

#Recode Columns

lapply(colnames(pt),summary)
colnames(pt)

install.packages("sqldf")
library(sqldf)

```

Note: the above loads the "small" data set. To load the full dataset instead, change the `in_path` to say "full" instead of `sample03`.

View the first few rows of some of the tables:

```{r}
head(scene_pt_tp_dim, n=10)
head(scene_mbr_dim, n=10)
```

```{r}

??sparkml



```


## Skeleton

#Milestone 1

#Tables/data to be used, 

Go forth with your analysis! Use `dplyr` to manipulate the data, `ggplot2` to plot/graph, `rpart` (or similar) for building classifiers, `cluster` for clustering, and all your other training. Good luck, we're all counting on you.