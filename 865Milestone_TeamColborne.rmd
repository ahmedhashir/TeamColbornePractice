---
title: "MMA 865 Final Project Milestone"
author: "Team Colborne"
date: "10th July 2017"
output:
  pdf_document: default
  html_document: default
---

#Project Overview

Team Colborne initiated the project by separating into two teams, one of which would explore the data in the R environemnt, the other would look into migrating the findings from the R environment into Spark through *sparklyr*. Early in the data exploration phase, the Spark team noticed techonological gaps between data filtering and evaluation capabilities in R vs. in *sparklyr*. Given the nature of the course as a **Big Data** course, we agreed that although the learning curve may be steeper, the team would commit to using *sparklyr*, acknowledging that data cleaning, filtering and troubleshooting documentation may be more difficult. 
The goal of Team Colborne for this project is to build a predictive model for any given months spending by customer, and to evaluate the factors that contribute to spending on the credit card. The business implications have been intentionally left out of this report because the intention is to derive these findings from the models, which have not yet been trained.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Packages

Given our intention to construct our models using Spark, the packages to be used for this project are limited to *rsparkling*, *H20*, *sparklyr*, and *dplyr*. A number of packages that are also included in *tidyverse* may be used on aggregate data once it has been collected into the R environment.


```{r libraries,echo = FALSE, message = FALSE, warning = FALSE}
rm(list=ls())
library(tidyverse)
library(data.table)
library(sparklyr)
library(rsparkling)
library(h2o)


```

#Loading the data

The 3 tables which are being used for analysis are *scene_mbr_dim*,*scene_mbr_acct_dim* and *scene_pt_fact*

```{r, echo = FALSE, warning=FALSE, message=FALSE, include=FALSE}


# Spark script
# config <- spark_config() 
# config$'sparklyr.shell.executor-memory' <- "20g" 
# config$'sparklyr.shell.driver-memory' <- "10g" 
# #config$spark.yarn.am.memory <- "15g"

# Full list of config options: https://spark.apache.org/docs/2.0.1/running-on-yarn.html
# 

#sc <- spark_connect(master = "yarn-client", spark_home = "/usr/hdp/current/spark2-client/", config=config)

#in_path_spark = 'hdfs:///user/hpc3552/scene-csv/sample0003/clean/'

#scene_mbr_dim <- spark_read_csv(sc, name='scene_mbr_dim', path=paste(in_path_spark, 'scene_mbr_dim.csv', sep=""), header = TRUE, delimiter = ",")

#scene_mbr_acct_dim <- spark_read_csv(sc, name='scene_mbr_acct_dim', path=paste(in_path_spark, 'scene_mbr_acct_dim.csv', sep=""), header = TRUE, delimiter = ",")

#scene_pt_fact <- spark_read_csv(sc, name='scene_pt_fact', path=paste(in_path_spark, 'scene_pt_fact.csv', sep=""), header = TRUE, delimiter = ",")

#R script
in_path_r = '/global/project/queens-mma/scene-csv/sample0003/clean/'

scene_mbr_dim <-fread(paste(in_path_r, 'scene_mbr_dim.csv', sep=""), sep=",")
scene_mbr_acct_dim <-fread(paste(in_path_r, 'scene_mbr_acct_dim.csv', sep=""), sep=",")
scene_pt_fact <-fread(paste(in_path_r, 'scene_pt_fact.csv', sep=""), sep=",")


```

# Selection of Variables and Joining the Tables

We have filtered for transactions that are between $0 and $10,000 to eliminate high-rollers and for transactions that were returned. Below are the box plots to indicate the outliers, this was used as substantiation to remove high rollers. The points highlighted in red in the box plots are the outliers, only the transaction amounts between $0 and $10000 are considered, elimating the other outliers.

```{r,echo = FALSE, warning=FALSE, message=FALSE, include=FALSE}
#First let's filter for transactions that are between 0 and 10k
scene_pt_fact_filter <- scene_pt_fact %>%
  filter(txn_amt > 0 & txn_amt <= 10000)
  
```


We are aware that our model loses some reliability given these filters. In future iterations, our model will reflect that some transactions that are greater than $0 were later reversed and will be omitted from the model.

The working code for joining the Spark tables is shown below

```{r}
#grouping by member key and arranging seq entry num in desc order so as to have the latest updated account entry

scene_dim1 <- scene_mbr_dim %>% 
  group_by(scene_mbr_key,scene_mbr_seq_num) %>%
  arrange(desc(scene_mbr_seq_num))%>%
  ungroup() 
 scene_dim2 <- scene_mbr_dim %>% 
  group_by(scene_mbr_key) %>%
  arrange(desc(scene_mbr_seq_num)) %>%
  summarise(scene_mbr_seq_num = max(scene_mbr_seq_num))
#an alternative to distinct function to delete duplicate entries  
scene_mbr_dim <- inner_join(scene_dim2,scene_dim1)

# Aggregating transaction amount by the month, year and trans type

#Initialize the dataframe that we'll use to create the lagged variables
scene_joined <- scene_pt_fact_filter%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "September")%>%
  group_by(scene_mbr_key)%>%
  summarise(sept_2016=sum(txn_amt))


#Now that we've created the data frame on which we'll join lagged variables, let's create a "loyalty" dataset that will only include transactions from the customers who spent money in september 2016. It will be computationally faster than using the big dataset for each join.
scene_loyalty_pt<-scene_pt_fact_filter%>%
  inner_join(y=scene_joined, by="scene_mbr_key")%>%
  select(-sept_2016)


#We want to omit the transaction type so that each customer has one entry per month
#txn_amt is the dollar value of transactions, cinema_pt_n is the number of transactions at a cinema
scene_loyalty_pt <- scene_loyalty_pt %>%
  group_by(scene_mbr_key, anul_clndr_code, mo_clndr_code) %>%
  summarise(txn_amt = sum(txn_amt), cin_pt_n = sum(txn_tp_3=="cin"))



#Now copy and paste the slightly altered code for each month and join it onto the original dataframe

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "August")%>%
  group_by(scene_mbr_key)%>%
  summarise(aug_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "July")%>%
  group_by(scene_mbr_key)%>%
  summarise(jul_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "June")%>%
  group_by(scene_mbr_key)%>%
  summarise(jun_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "May")%>%
  group_by(scene_mbr_key)%>%
  summarise(may_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "April")%>%
  group_by(scene_mbr_key)%>%
  summarise(apr_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "March")%>%
  group_by(scene_mbr_key)%>%
  summarise(mar_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "February")%>%
  group_by(scene_mbr_key)%>%
  summarise(feb_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code == "January")%>%
  group_by(scene_mbr_key)%>%
  summarise(jan_2016=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

#Now let's do it for 2015 variables

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "December")%>%
  group_by(scene_mbr_key)%>%
  summarise(dec_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "November")%>%
  group_by(scene_mbr_key)%>%
  summarise(nov_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "October")%>%
  group_by(scene_mbr_key)%>%
  summarise(oct_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "September")%>%
  group_by(scene_mbr_key)%>%
  summarise(sep_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "August")%>%
  group_by(scene_mbr_key)%>%
  summarise(aug_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "July")%>%
  group_by(scene_mbr_key)%>%
  summarise(jul_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "June")%>%
  group_by(scene_mbr_key)%>%
  summarise(jun_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "May")%>%
  group_by(scene_mbr_key)%>%
  summarise(may_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "April")%>%
  group_by(scene_mbr_key)%>%
  summarise(apr_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "March")%>%
  group_by(scene_mbr_key)%>%
  summarise(mar_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "February")%>%
  group_by(scene_mbr_key)%>%
  summarise(feb_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code == "January")%>%
  group_by(scene_mbr_key)%>%
  summarise(jan_2015=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2014 & mo_clndr_code == "December")%>%
  group_by(scene_mbr_key)%>%
  summarise(dec_2014=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2014 & mo_clndr_code == "November")%>%
  group_by(scene_mbr_key)%>%
  summarise(nov_2014=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2014 & mo_clndr_code == "October")%>%
  group_by(scene_mbr_key)%>%
  summarise(oct_2014=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2014 & mo_clndr_code == "September")%>%
  group_by(scene_mbr_key)%>%
  summarise(sep_2014=sum(txn_amt))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

#Now let's look at the number of transactions in 6-month blocks and join it onto scene_joined

#Filter for the past 6 months spend and aggregate it into one variable
scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2016 & mo_clndr_code %in% c("August", "July", "June", "May", "April", "March"))%>%
  group_by(scene_mbr_key)%>%
  summarise("num_cin_6"=sum(cin_pt_n))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

##Filter for months t-12 to t-7 and aggregate it into one variable
# | is the logical equivalent of "or"
scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code ==2016 & mo_clndr_code %in% c("February", "January") | anul_clndr_code == 2015 & mo_clndr_code %in% c("December", "November", "October", "September"))%>%
  group_by(scene_mbr_key)%>%
  summarise("num_cin_12"=sum(cin_pt_n))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

##Filter for months t-18 to t-13 and aggregate it into one variable
scene_joined<-scene_loyalty_pt%>%
  filter(anul_clndr_code == 2015 & mo_clndr_code %in% c("August", "July", "June", "May", "April", "March"))%>%
  group_by(scene_mbr_key)%>%
  summarise("num_cin_18"=sum(cin_pt_n))%>%
  right_join(y=scene_joined, by="scene_mbr_key")

#okay now lets join it onto the demo data.

scene_columns <- c("scene_mbr_key",
                   "scene_mbr_seq_num",
                   "eff_from_tmstamp",
                   "eff_to_tmstamp","brth_dt",
                   "psnl_post_cd","psnl_prov_state_cd","psnl_city",  
                   "suspended_f",
                   "gndr_desc",
                   "prefrd_loctn_desc",
                   "email_prefnc_desc",
                   "ed_lvl_desc",
                   "prefrd_show_tm_desc",
                   "num_of_hh_pple_desc",
                   "movie_gng_frq_ref_desc",
                   "mrtl_stat_desc",
                   "lang_desc",
                   "scene_acty_stat")


#Join Customer Dimension Data with scene_data
# Excluding reference key columns from joined data
# scene_data<- inner_join(x = scene_mbr_dim, 
#                     y = scene_joined, 
#                     by = "scene_mbr_key")%>%
#               select(one_of(scene_columns))

# Joining scene_data with account data
scene_data <- scene_mbr_dim%>%
                  select(one_of(scene_columns))%>%
              inner_join(y=scene_joined, by = "scene_mbr_key")
  
```

#Feature Engineering
Decision to keep location data and omit some highly correlated variables from datasets for different types of models. 

```{r,echo = FALSE, warning=FALSE, message=FALSE, include=FALSE}
#Converting birth year to age
scene_data<- scene_data %>% 
  mutate(Age = 2017 - brth_dt)%>%
  select(-brth_dt)
  
```

```{r}
scene_data <- scene_data %>%
  select(
     -scene_mbr_key,
    -scene_mbr_seq_num,
    -eff_from_tmstamp,
    -eff_to_tmstamp
        )




```


#Use of h2o because some other feature engineering can't be done in Spark

```{r,echo = FALSE, message = FALSE, warning = FALSE}

###Below is the code for creating monthly variables for spend by each customer. This code will not work in sparklyr or on an h2o class data.table so we need to figure out a new way to integrate time series effects

# scene_data_lag<-scene_data%>%
#   unite("MoYear", mo_clndr_code, anul_clndr_code, sep="")
# scene_data_lag<-spread(scene_data_lag, MoYear, txn_amt, fill= 0)



#Now we'll try it on an h2o data table

# scene_long<-scene_data%>%
#   select(one_of(c("scene_mbr_key", "anul_clndr_code", "txn_amt")))

# scene_long<-as.h2o(scene_long)
# 
# h2o.pivot(x= scene_long, index= "scene_mbr_key", column = "anul_clndr_code", value= "txn_amt")


```


#Models

Team Colborne decided to use H2O as their machine learning library from the outset so that the models could be easily transferred to the *sparklyr* environment using the package *rsparkling*. Preliminary research has indicated H2O has several key advantages over Spark MLlib, namely:

- Benchmark testing has indicated that H2O is significantly faster. Given the limitations of the Spark cluster we've been provided, speeding up our learning models will allow us to iterate quickly.

- H2O provides a common interface for training and evaluating the machine learning algorithms, similar to what is available in the *caret* package. Given that this will be our first exposure to H2O, we believe this will allow us to quickly learn the syntax and apply it across numerous models.

- Models such as deep learning, ensemble stacking, and others, are not yet available in Spark MLlib. The Team is interested in experimenting with these approaches, and this would not be possible without H2O.

For modeling purposes, we have identified three candidate models that we believe will provide a necessary baseline from which to work. More specifically, we have selected GLM using the Elastic Net family, Random Forest, and Gradient Boosted Machines. 

Given that elastic nets are able to perform variable selection and the models are interpretable, we are planning to use the elastic net model in our presentation of our findings. We should be able to talk about effect sizes and the significance of the variables that we have selected. However, we acknowledge that is unlikely to be the best model.

As a result, we have decided to augment our business insights with models that have more predictive power. We plan to use the variable importance function to confirm the validity of the variables that were selected from elastic nets, and then use the models we create from RF and GBM to deliver more accurate predictions.

The following code chunk presents the skeleton for the H2O models. 

```{r}
# For the purposes of this example, we are assuming the final data frame will be held in a Spark dataframe called scene_data.

######## Following Skeleton Code may not completely run as of now #######

h2o.init(nthreads = -1)

scene_data <- as.h2o(scene_data)

#Check out the structure to confirm it converted properly.

str(scene_data)

# Convert factor variables
# Note: This is not possible in sparklyr, but works perfectly in H2O!

names <- names(scene_data)
char_names <- names[is.character(scene_data[,names])]


scene_data[,char_names] <- as.factor(scene_data[,char_names])

# Similar to caret, h2o givesus an easy way to build training / testing sets

# To get a validation dataset, we simply enter 0.6, 0.3. The remaining 0.1 is implicitly defined.

# Given that we're planning on using lagged variables, we're hoping the time nature of the prediction problem won't be an issue. However, we may need to find alternative ways to split our data if it doesn't work.

splits <- h2o.splitFrame(scene_data, 0.8)
train <- splits[[1]]
test <- splits[[2]]

# Tell h2o which are response variables and which are features
y <- "sept_2016"  
x <- names[names(scene_data)!=y]


# Building the model! 
# nfolds tells it to use 10 fold cross-validation, note that we won't be doing this on the large data set, just on the sample.

# RF_Defaults is the default random forest, using ntrees = 50 and maxdepth = 20
# We have set it up so we can do hyperparameter tuning afterwards

m_rf_default <- h2o.randomForest(x, y, train, nfolds = 10, model_id = "RF_defaults")

# Show the performance on the test set
h2o.performance(m_rf_default, test)

# Here's how we plan to do parameter tuning!
g_rf <- h2o.grid("randomForest",
              hyper_params = list(
                ntrees = c(50, 100, 120),
                max_depth = c(40, 60),
                min_rows = c(1, 2)
              ),
              x = x, y = y, training_frame = train, nfolds = 10
)

rf_grid <- h2o.getGrid(g_rf@grid_id, sort_by = "r2", decreasing = TRUE)
best_rf_model_id <- rf_grid@model_ids[[1]]
m_rf_tuned <- h2o.getModel(best_rf_model_id)

# Build the GBM skeleton

m_gbm_default <- h2o.gbm(x, y, train, nfolds = 10, model_id = "GBM_defaults")

h2o.performance(m_gbm_default, test)

# Parameter tuning for GBM

g_gbm <- h2o.grid("gbm",
              hyper_params = list(
                ntrees = c(50, 100, 120),
                max_depth = c(40, 60),
                min_rows = c(1, 2),
                sample_rate = c(0.7, 0.8, 0.9, 1),
                col_sample_rate = c(0.7, 0.9, 1),
                nbins = c(8, 12, 16, 20, 24, 28, 32),
                stopping_tolerance = 0.001
              ),
              x = x, y = y, training_frame = train, nfolds = 10
)

gbm_grid <- h2o.getGrid(g_gbm@grid_id, sort_by = "r2", decreasing = TRUE)
best_gbm_model_id <- gbm_grid@model_ids[[1]]
m_gbm_tuned <- h2o.getModel(best_gbm_model_id)

# Finally, skeleton for the elastic net.

m_glm_default <- h2o.glm(x, y, train, nfolds = 10, model_id = "GLM_defaults")

g_glm <- h2o.grid("glm",
              hyper_params = list(
                alpha = c(0, 0.25, 0.5, 0.75, 1),
                lambda = c(0, 0.25, 0.5, 0.75, 1)
              ),
              x = x, y = y, training_frame = train, nfolds = 10
)

glm_grid <- h2o.getGrid(g_glm@grid_id, sort_by = "r2", decreasing = TRUE)
best_glm_model_id <- glm_grid@model_ids[[1]]
m_glm_tuned <- h2o.getModel(best_glm_model_id)

```

```{r,echo = FALSE, message = FALSE, warning = FALSE}
###Plots of txn_amt to indicate outliers

#boxplot of txn_amt for the original transaction data
ggplot(scene_pt_fact, aes(anul_clndr_code,txn_amt))+
  geom_boxplot(outlier.colour = "red", outlier.shape = 1)+
  labs(x = "year",
         y = "transaction amount",
         title = "Plot for the original transaction data")
 

#boxplot of txn_amt after filtering transaction data
ggplot(scene_pt_fact_filter, aes(anul_clndr_code,txn_amt))+
  geom_boxplot(outlier.colour = "red", outlier.shape = 1)+ 
  labs(x = "year",
         y = "transaction amount",
         title = "Plot for the filtered transaction data")

#histogram of txn_amt 
ggplot(scene_pt_fact_filter, aes(txn_amt))+
  geom_histogram(binwidth = 300)+
  labs(x = "transaction amount",
         title = "Histogram for transaction amount")
```
